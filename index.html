<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Simon Wanna | Portfolio</title>
    <link rel="stylesheet" href="assets/css/style.css" />
    <link rel="icon" type="image/x-icon" href="assets/favicon.ico" />
  </head>
  <body>
    <div class="container">
      <!-- Bio Section -->
      <header>
        <div style="display: flex; align-items: center; gap: 30px; margin-bottom: 2rem;">
            <img src="/assets/img/logo.jpg" alt="Profile" style="width: 120px; height: 120px; border-radius: 50%; object-fit: cover;">
            <h1>Welcome to my personal website!</h1>
        </div>
        
        <div class="bio-text">
          <p>
            From the age of eight to twenty-three, swimming taught me focus,
            discipline, and the ability to delay gratification. Growing up in a
            pizzeria taught me humility, and that everyone carries their own
            struggles. Curiosity taught me more skills than university ever did,
            but there I met people I learned even more from. I also got more
            immersed in science, which interests me a lot, because it reflects
            the pursuit of the next level and that's always where I want to go.
          </p>
          <p><br />If you're on a similar path, I'd be glad to connect.</p>

          <div class="social-links" style="margin-top: 20px; margin-bottom: 30px;">
            <a href="https://github.com/simonwanna" target="_blank" class="badge" style="font-size: 1rem; padding: 8px 16px; margin-right: 10px; display: inline-flex; align-items: center; gap: 8px;">
              <svg height="20" width="20" viewBox="0 0 16 16" fill="white">
                <path d="M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59.4.07.55-.17.55-.38 0-.19-.01-.82-.01-1.49-2.01.37-2.53-.49-2.69-.94-.09-.23-.48-.94-.82-1.13-.28-.15-.68-.52-.01-.53.63-.01 1.08.58 1.23.82.72 1.21 1.87.87 2.33.66.07-.52.28-.87.51-1.07-1.78-.2-3.64-.89-3.64-3.95 0-.87.31-1.59.82-2.15-.08-.2-.36-1.02.08-2.12 0 0 .67-.21 2.2.82.64-.18 1.32-.27 2-.27.68 0 1.36.09 2 .27 1.53-1.04 2.2-.82 2.2-.82.44 1.1.16 1.92.08 2.12.51.56.82 1.27.82 2.15 0 3.07-1.87 3.75-3.65 3.95.29.25.54.73.54 1.48 0 1.07-.01 1.93-.01 2.2 0 .21.15.46.55.38A8.013 8.013 0 0016 8c0-4.42-3.58-8-8-8z"></path>
              </svg>
              GitHub
            </a>
            <a href="https://www.linkedin.com/in/simonwanna/" target="_blank" class="badge" style="font-size: 1rem; padding: 8px 16px; display: inline-flex; align-items: center; gap: 8px;">
              <svg height="20" width="20" viewBox="0 0 24 24" fill="white">
                <path d="M20.447 20.452h-3.554v-5.569c0-1.328-.027-3.037-1.852-3.037-1.853 0-2.136 1.445-2.136 2.939v5.667H9.351V9h3.414v1.561h.046c.477-.9 1.637-1.85 3.37-1.85 3.601 0 4.267 2.37 4.267 5.455v6.286zM5.337 7.433c-1.144 0-2.063-.926-2.063-2.065 0-1.138.92-2.063 2.063-2.063 1.14 0 2.064.925 2.064 2.063 0 1.139-.925 2.065-2.064 2.065zm1.782 13.019H3.555V9h3.564v11.452zM22.225 0H1.771C.792 0 0 .774 0 1.729v20.542C0 23.227.792 24 1.771 24h20.451C23.2 24 24 23.227 24 22.271V1.729C24 .774 23.2 0 22.222 0h.003z"></path>
              </svg>
              LinkedIn
            </a>
          </div>

          <!-- <div class="principles">
            <h3>Principles I live by:</h3>
            <br />
            <div class="principle-item">
              <div class="principle-title">üß≠ Forge your own path</div>
              <div>
                Disregard the ‚Äúcommon route‚Äù in life, you are not like the
                people who recommended you that path.
              </div>
            </div>
            <div class="principle-item">
              <div class="principle-title">üöÄ Pursue meaningful journeys</div>
              <div>
                Set ambitious goals, but make sure they are dependent on a
                stimulating journey, you will be spending a lot of time on it
                and life is short.
              </div>
            </div>
            <div class="principle-item">
              <div class="principle-title">‚è≥ Mastery takes time</div>
              <div>
                The 10,000-hour saying might be an oversimplified
                generalisation, but I never seen a truly impressive person who
                hasn't put in a lot of hours.
              </div>
            </div>
          </div> -->
        </div>
      </header>

      <!-- Latest Experience -->
      <section class="latest-section">
        <div class="latest-info">
          <h3>Current Position: Master's Thesis @ Einride</h3>
          <p>Verification and Validation Team (Jan 2025 - June 2026)</p>
        </div>
        <a href="cv.html" class="cv-button">View Full CV</a>
      </section>

      <!-- Projects Grid -->
      <h3 style="margin-bottom: 30px; font-weight: 600; color: #fff;">Projects</h3>
      <section class="projects-grid">
        <!-- Grandi -->
            <div class="project-card" data-id="grandi">
          <img
            src="/assets/portfolio/grandi.png"
            alt="Grandi"
            class="project-thumb"
          />
          <div class="project-title">Grandmaster Intuition</div>
        </div>

        <!-- RaySR -->
            <div class="project-card" data-id="raysr">
          <img
            src="/assets/portfolio/sr-ericsson.png"
            alt="RaySR"
            class="project-thumb"
          />
          <div class="project-title">RaySR @ Ericsson</div>
        </div>

        <!-- CalliQ -->
            <div class="project-card" data-id="calliq">
          <img
            src="/assets/portfolio/hackathon.jpg"
            alt="CalliQ"
            class="project-thumb"
          />
          <div class="project-title">CalliQ @ Microsoft Hackathon</div>
        </div>

        <!-- AD Prediction -->
            <div class="project-card" data-id="ad-prediction">
          <img
            src="/assets/portfolio/itsc.png"
            alt="AD Prediction"
            class="project-thumb"
          />
          <div class="project-title">AD Prediction Research</div>
        </div>

        <!-- EyeBall -->
            <div class="project-card" data-id="eyeball">
          <img
            src="/assets/portfolio/eyeball-gameplay.png"
            alt="EyeBall"
            class="project-thumb"
          />
          <div class="project-title">EyeBall RL Game</div>
        </div>

        <!-- Robotics -->
            <div class="project-card" data-id="robotics">
                    <img
            src="/assets/portfolio/interplan_thumb.png"
            alt="Robotics"
            class="project-thumb"
          />
          <div class="project-title">Robotics Course Project</div>
        </div>

            <div class="project-card" data-id="thesis">
          <img
            src="/assets/portfolio/nuplan_thumb.png"
            alt="Bachelor Thesis"
            class="project-thumb"
          />
          <div class="project-title">Bachelor Thesis @ Scania</div>
        </div>

            <div class="project-card" data-id="s2g">
          <img
            src="/assets/portfolio/s2g_thumb.png"
            alt="Speech to Gesture"
            class="project-thumb"
          />
          <div class="project-title">Speech to Gesture</div>
        </div>
      </section>

      <!-- Hidden Content for Modals -->
      <div style="display: none">
        <!-- Grandi -->
            <div id="content-grandi">
          <img src="/assets/portfolio/grandi.png" />
          <p><strong>Dec 2025</strong></p>
          <p>Grandi is a chess engine designed to mimic "Grandmaster Intuition" by predicting win probabilities from a single board state, rather than searching millions of future paths like traditional engines. Inspired by AlphaZero, the model uses a deep CNN with 18 binary feature planes to "see" strategic elements. I built a web interface with Gradio and deployed it on Hugging Face Spaces, allowing users to play against an engine and analyze their moves in real-time.</p>
          <br>
          <p>I also built a fully automated MLOps pipeline on Google Cloud Platform to keep the engine learning. I configured game logs to stream to BigQuery, triggering a weekly GitHub Actions workflow that fine-tunes the model on new amateur game data, adapting the system to gameplay styles not present in the original training data. I hosted the inference API on Cloud Run using FastAPI, implementing a dynamic reloading mechanism for zero-downtime updates, and added a monitoring layer in Looker Studio to track performance drift.</p>
          <br />
          <a
            href="https://github.com/simonwanna/grandi"
            target="_blank"
            class="badge"
            >GitHub Repository</a
          >
        </div>

        <!-- RaySR -->
            <div id="content-raysr">
          <img src="/assets/portfolio/sr-ericsson.png" />
          <p><strong>Sep 2025 - Dec 2025</strong></p>
          <p>This project explored the use of Super Resolution (SR) techniques to improve generation of radio maps, which are used to predict the signal strength of a radio signal in a given location. Inspired by NVIDIA's DLSS, the project aimed to see if a similar result can be achieved for radio maps, i.e. to generate a low resolution radio map and upsample it, faster than only using a high resolution radio map, without losing essential quality.</p>
          <br>
          <p>I worked on this project as part of the course DD2430 at KTH, in collaboration with Ericsson and fellow classmates. I utilized the Sionna physics simulator to explore techniques like ray tracing for data generation and applied SR models to the output. The project showed clear improvements in generation speed without losing essential quality. However, one limiting factor was the low complexity in the scenes used, and the requirements from network operators must be evaluated more closely to see if it is a viable solution.</p>
          <br />
          <a href="/assets/pdf/raysr-report.pdf" target="_blank" class="badge"
            >Please View Report (PDF)</a
          >
          <a
            href="https://github.com/simonwanna/RaySR"
            target="_blank"
            class="badge"
            >GitHub Repository</a
          >
        </div>

        <!-- CalliQ -->
            <div id="content-calliq">
          <img src="/assets/portfolio/hackathon.jpg" />
          <p><strong>May 2025</strong></p>
          <p>Together with <a href="https://www.linkedin.com/in/didrikmunther/" target="_blank">Didrik Munther</a>, I built CalliQ (Call IQ) which is an assistant designed to prevent phone scams. The agent listens in on phone calls in real time, transcribes them using Microsoft‚Äôs Azure Speech SDK, and leverages a LangGraph LLM agent to detect and warn the receiver if a scam attempt is suspected.</p>
          <br>
          <p>We developed and demoed the prototype by conducting a live phone call between two devices on stage during the SSE x KTH x Microsoft Hackathon at Microsoft HQ in Stockholm. Out of 300 applicants, 14 teams were selected to build and pitch their projects, and ours was awarded runner-up.</p>
          <br />
          <a
            href="https://www.linkedin.com/feed/update/urn:li:activity:7328041781772632065/"
            target="_blank"
            class="badge"
            >SSE LinkedIn Post</a
          >
        </div>

        <!-- AD Prediction -->
            <div id="content-ad-prediction">
          <img src="/assets/portfolio/itsc.png" />
          <p><strong>Sep 2024 - May 2025</strong></p>
          <p>This project focused on improving trajectory prediction for autonomous driving by exploring ensemble methods to better handle difficult, long-tailed cases. Working with Oxford Robotics, I implemented and tested ensembles on large-scale datasets such as <a href="https://www.nuscenes.org/" target="_blank">nuScenes</a>.</p>
          <br>
          <p>The project explored distillation and other techniques but surprisingly, the most effective approach turned out to be a simple confidence-weighted average of pre-trained models, without retraining or fine-tuning. This yielded up to 10% improvement in prediction accuracy, particularly in the hardest long-tail scenarios, compared to any single model. The work also revealed how different models complement each other: smaller models, though weaker on average, provided robustness in edge cases where larger models failed.</p>
          <br>
          <p>The project culminated in a paper accepted to the IEEE International Conference on Intelligent Transportation Systems (ITSC) 2025 and was selected for an oral presentation.</p>
          <br />
          <a
            href="https://arxiv.org/abs/2509.13914"
            target="_blank"
            class="badge"
            >arXiv Paper</a
          >
          <a
            href="https://github.com/dthuremella/Ensemble-of-Pretrained-Models"
            target="_blank"
            class="badge"
            >GitHub Repository</a
          >
        </div>

        <!-- EyeBall -->
            <div id="content-eyeball">
          <img src="/assets/portfolio/eyeball-gameplay.png" />
          <p><strong>Mar 2025</strong></p>
          <p>EyeBall is a game where you control a ball with your eyes while competing on a track against a reinforcement learning (RL) agent. The idea was to combine computer vision with interactive gameplay and to use the setup as a practical playground to learn more about reinforcement learning.</p>
          <br>
          <p>To achieve this, I built an eye-tracking interface using MediaPipe FaceMesh that detects gaze direction in real time and maps it to in-game lane switching. On top of this, I created a game environment in Ursina that could be played either by the user (via eye movements or keyboard) or by an RL agent. For the agent, I designed a headless version of the environment and trained it with PPO from Stable-Baselines3, experimenting with reward shaping for lane changes, obstacle avoidance, and survival time.</p>
          <br />
          <a
            href="https://github.com/simonwanna/EyeBall"
            target="_blank"
            class="badge"
            >GitHub Repository</a
          >
        </div>

        <!-- Robotics -->
            <div id="content-robotics">
          <video controls autoplay loop muted playsinline width="100%">
            <source src="/assets/portfolio/interplan.mp4" type="video/mp4" />
          </video>
          <p><strong>Jan 2025 - May 2025</strong></p>
          <p>This project focused on improving trajectory planning in autonomous driving, with emphasis on out-of-distribution (OOD) generalization.</p>
          <br>
          <p>I built upon a baseline model that generated multiple trajectories and ranked them, but which consistently failed in OOD cases. I experimented with incorporating large-scale representation learning through LLM and VLM embeddings in PyTorch Lightning. The hypothesis was that such multimodal embeddings, with richer "world knowledge", could support better reasoning in edge cases. LLM-based embeddings were difficult because each dynamic scene had to be translated into a sufficiently rich textual description; a VLM embedding using vehicle history and scene context was simpler and more robust, though gains remained scenario-dependent.</p>
          <br>
          <p>My modifications to the original baseline model enabled the system to resolve some of these edge cases like driving around a blocking car and return safely to the correct lane (shown in the simulation above), and with additional training tweaks I also achieved a higher score than the reported SOTA on the <a href="https://github.com/mh0797/interPlan" target="_blank">InterPlan</a> dataset. However, the simulator scoring itself proved noisy and gave somewhat questionable metrics.</p>
          <br>
          <p>The project was part of the course <a href="https://www.kth.se/student/kurser/kurs/DD2414?l=en" target="_blank">DD2414</a> Engineering project in Robotics, Perception and Learning (15 credits).</p>
        </div>

        <!-- Thesis -->
            <div id="content-thesis">
          <video controls autoplay loop muted playsinline width="100%">
            <source src="/assets/portfolio/nuplan-sim.mp4" type="video/mp4" />
          </video>
          <p><strong>Jan 2024 - June 2024</strong></p>
          <p>This thesis project on autonomous driving was done in collaboration with <a href="https://www.linkedin.com/in/tomboustedt/" target="_blank">Tom Boustedt</a> and it spanned everything from data processing to simulator work, model building, and evaluation using PyTorch, Weights and Biases and the <a href="https://www.nuscenes.org/nuplan" target="_blank">nuPlan</a> simulator, all done on AWS.</p>
          <br>
          <p>The core research focused on a key question: could we improve autonomous driving performance by integrating and simultaneously optimizing the predictor with the planner in a differentiable manner? Rather than treating prediction and planning as separate modules, the project explored whether joint optimization could lead to better overall system performance.</p>
          <br>
          <p>The approach also integrated a non-linear optimization layer built using <a href="https://research.facebook.com/publications/theseus-a-library-for-differentiable-nonlinear-optimization/" target="_blank">Theseus</a> to enforce kinematic driving constraints. This created a more transparent system that could aid safe development compared to previous black-box approaches.</p>
          <br>
          <p>The results did not reach SOTA on overall performance, but in some scenarios where previous models struggled, our integrated approach showed improved lane-keeping behavior and better collision avoidance. We continued refining the model after thesis submission and presented the enhanced results at Scania in June 2024.</p>
          <br />
          <a
            href="https://kth.diva-portal.org/smash/get/diva2:1880732/FULLTEXT01.pdf"
            target="_blank"
            class="badge"
            >Bachelor Thesis PDF</a
          >
        </div>

        <!-- S2G -->
            <div id="content-s2g">
          <video controls autoplay loop muted playsinline width="100%">
            <source
              src="/assets/portfolio/s2g-zeggs-rig.mp4"
              type="video/mp4"
            />
          </video>
          <p><strong>Feb 2023 - Jan 2024</strong></p>
          <p>This project aimed to explore speech-to-gesture generation using deep learning. The goal was to see if we could curate a dataset of different motion capture data (different sizes, number of joints etc.) from sources like <a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS" target="_blank">Ubisoft ZeroEGGS dataset</a> and map the audio features directly to human motion. Therefore, I spent a good amount of time in Blender, retargeting the motion capture data to align everything with a unified rig across datasets.</p>
          <br>
          <p>Our modelling approach focused on extracting Mel spectrograms and frequency domain features using PyTorch Audio, testing the hypothesis that information in the frequency domain could map to acceleration in joint space to predict realistic motion based on speech. We started building a VAE model to handle the complexity of human gesture generation.</p>
          <br>
          <p>Although the project was discontinued when my thesis began, it became my introduction to serious research. Working with Linux environments, high‚Äëperformance clusters, PyTorch, and signal processing gave me a solid grounding in multimodal deep learning and the challenges of real‚Äëworld motion data.</p>
        </div>
      </div>

      <!-- Extracurriculars -->
      <div class="extra-section-wrapper">
        <h3 style="margin-bottom: 30px; font-weight: 600; color: #fff;">Extracurriculars</h3>
        <section class="extra-section">
          <div class="extra-item">
            <h3>Swimming</h3>
            <p>
              Competed for 15 years. National team level in open water marathon
              swimming.
            </p>
            <p><strong>Winner of Vansbrosimningen 2015</strong></p>
            <p><strong>OW Swimmer of the Year 2016</strong></p>
          </div>
          <div class="extra-item">
            <h3>Music</h3>
            <p>Self-taught Guitar (8y) & Piano (2y).</p>
            <p>Composed: <em>Recursive Solace</em></p>
            <div class="audio-card" style="margin-top: 10px">
              <audio controls>
                <source
                  src="/assets/audio/RecursiveSolace.mp3"
                  type="audio/mpeg"
                />
              </audio>
            </div>
          </div>
        </section>
      </div>
    </div>

    <!-- Modal Container -->
    <div id="project-modal" class="modal">
      <div class="modal-content">
        <span class="close-modal">&times;</span>
        <h2 id="modal-title" style="margin-bottom: 20px"></h2>
        <div id="modal-body" class="modal-body"></div>
      </div>
    </div>

    <script src="assets/js/script.js"></script>
  </body>
</html>
